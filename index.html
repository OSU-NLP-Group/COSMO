<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="DESCRIPTION META TAG">
    <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
    <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
    <meta property="og:url" content="URL OF THE WEBSITE" />
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/image/your_banner_image.png" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />


    <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
    <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
    <meta name="twitter:card" content="summary_large_image">
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>COSMO</title>
    <!-- <link rel="icon" type="image/x-icon" href="static/images/mammoth_icon.png"> -->
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <script src="static/js/jquery.min.js"></script>
    <script src="static/js/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>

    <link rel="stylesheet" type="text/css" href="static/css/jquery.dataTables.css">
    <script type="text/javascript" charset="utf8" src="static/js/jquery-3.5.1.js"></script>
    <script type="text/javascript" charset="utf8" src="static/js/jquery.dataTables.js"></script>
</head>

<body>


    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">Bringing Back the Context: Camera Trap Species Identification as Link Prediction on Multimodal Knowledge Graphs</h1>
                        <div class="is-size-5 publication-authors">
                            <!-- Paper authors -->
                            <span class="author-block">
                            <a href="https://vardaan123.github.io/" target="_blank">Vardaan Pahuja</a><sup>1</sup>,
                            </span>
                            <span class="author-block">
                            <a href="https://www.linkedin.com/in/eddy-luo-aa02b5242/" target="_blank">Weidi Luo</a><sup>1</sup>,
                            </span>
                            <span class="author-block">
                            <a href="https://entslscheia.github.io/" target="_blank">Yu Gu</a><sup>1</sup>,
                            </span>
                            <span class="author-block">
                            <a href="https://andytu28.github.io/" target="_blank">Cheng-Hao Tu</a><sup>1</sup>,
                            </span>
                            <span class="author-block">
                            <a href="https://sites.google.com/view/hongyouc/" target="_blank">Hong-You Chen</a><sup>1</sup>,
                            </span>
                            <span class="author-block">
                            <a href="https://cse.osu.edu/people/berger-wolf.1" target="_blank">Tanya Berger-Wolf</a><sup>1</sup>,
                            </span>
                            <span class="author-block">
                            <a href="https://www.cs.rpi.edu/~stewart/" target="_blank">Charles Stewart</a><sup>2</sup>,
                            </span>
                            <span class="author-block">
                            <a href="https://geography.wisc.edu/staff/gao-song/" target="_blank">Song Gao</a><sup>3</sup>,
                            </span>
                            <span class="author-block">
                            <a href="https://sites.google.com/view/wei-lun-harry-chao" target="_blank">Wei-Lun Chao</a><sup>1</sup>,
                            </span>
                            <span class="author-block">
                            <a href="https://ysu1989.github.io/" target="_blank">Yu Su</a><sup>1</sup>,
                            </span>


                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                    <sup>1</sup>The Ohio State University,
                    <sup>2</sup>Rensselaer Polytechnic Institute, 
                    <sup>3</sup>University of Wisconsin-Madison, 
                            <!-- <span class="author-block"><a href="mailto:pahuja.9@osu.edu">pahuja.9@osu.edu</a> -->

                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">

                                <!-- Github link -->
                                <span class="link-block">
                      <a href="https://github.com/OSU-NLP-Group/COSMO" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                                <span>Code</span>
                                </a>
                                </span>

                                <!-- ArXiv abstract Link -->
                                <span class="link-block">
                    <a href="https://arxiv.org/pdf/2401.00608.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                                <span>arXiv</span>
                                </a>
                                </span>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>



    <!-- Paper abstract -->
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h2 class="title is-3">Abstract</h2>
                        <div class="content has-text-justified">
                            <p>
                              Camera traps are valuable tools in animal ecology for biodiversity monitoring and conservation.
                              However, challenges like poor generalization to deployment at new unseen locations limit their practical application.
                              Images are naturally associated with heterogeneous forms of context possibly in different modalities.
                              In this work, we leverage the structured context associated with the camera trap images to improve out-of-distribution generalization for the task of species identification in camera traps.
                              For example, a photo of a wild animal may be associated with information about where and when it was taken, as well as structured biology knowledge about the animal species. 
                              While typically overlooked by existing work, bringing back such context offers several potential benefits for better image understanding, such as addressing data scarcity and enhancing generalization.
                              However, effectively integrating such heterogeneous context into the visual domain is a challenging problem.
                              To address this, we propose a novel framework that reformulates species classification as link prediction in a multimodal knowledge graph (KG).
                              This framework seamlessly integrates various forms of multimodal context for visual recognition.
                              We apply this framework for out-of-distribution species classification on the iWildCam2020-WILDS and Snapshot Mountain Zebra datasets and achieve competitive performance with state-of-the-art approaches. Furthermore, our framework successfully incorporates biological taxonomy for improved generalization and enhances sample efficiency for recognizing under-represented species.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <!-- End paper abstract -->



    <!-- Image carousel -->
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <!-- <div class="column is-four-fifths"> -->
                        <div class="item">
                            <!-- Your image here -->
                            <img src="static/images/mmkg_model_combined.png" alt="Overview of COSMO framework" />
                            <h2 class="subtitle">
                                <b>Overview of our framework COSMO</b>. <b><i>Left</b></i>: Our multimodal knowledge graph for camera traps and wildlife. Photos from
camera traps are jointly represented in the KG with contextual information such as time, location, and structured biology taxonomy. <b><i>Right</b></i>: In our formulation of species classification as link prediction, the plausibility score Ïˆ(s, r, o) of each (subject, relation, object) triple is computed using a KGE model (<i>e.g.</i>, DistMult), where the subject, relation, and object are all first embedded into a vector space. 
                            </h2>
                        </div>
                    <!-- </div> -->
                </div>
            </div>
        </div>
    </section>
    <!-- End image carousel -->


    <section class="hero is-small">
        <div class="hero-body">
            <div class="container  is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">Overall Results</h2>
                        <br>
                        <div class="item">
                            <img src="static/images/iwildcam_results_table.png" alt="Species Classification results on iWildCam2020-WILDS (OOD) dataset" />
                            <!-- <embed src="static/images/iwildcam_results_table.pdf" alt="Species Classification results on iWildCam2020-WILDS (OOD) dataset" width="2677px" height="1890px"/> -->
                            <!-- <h2 class="subtitle has-text-centered"> -->
                                <!-- Figure 2: Overall results of ðŸ¦£MAmmoTH on the in-domain and out-of-domain datasets. -->
                            <!-- </h2> -->
                            <p>
                                Species Classification results on iWildCam2020-WILDS (OOD) dataset. The first baseline in the second section shows the
                                no-context baseline that uses only image-species labels as KG edges. All models use a pre-trained ResNet-50 as image encoder. Parentheses show standard deviation across 3 random seeds. Missing values are denoted by â€“.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="hero is-small">
        <div class="hero-body">
            <div class="container  is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <!-- <div class="column is-four-fifths"> -->
                        <!-- <h2 class="title is-3">Overall Results</h2> -->
                        <div class="item">
                            <img src="static/images/snapshot_mountain_zebra_results_table.png" alt="Species Classification results on Snapshot Mountain Zebra dataset" />
                            <!-- <embed src="static/images/iwildcam_results_table.pdf" alt="Species Classification results on iWildCam2020-WILDS (OOD) dataset" width="2677px" height="1890px"/> -->
                            <!-- <h2 class="subtitle has-text-centered"> -->
                                <!-- Figure 2: Overall results of ðŸ¦£MAmmoTH on the in-domain and out-of-domain datasets. -->
                            <!-- </h2> -->
                            <p>
                                Species Classification results on Snapshot Mountain Zebra dataset.
                            </p>
                        </div>
                    <!-- </div> -->
                </div>
            </div>
        </div>
    </section>


    <!-- <section class="hero is-small">
        <div class="hero-body">
            <div class="container  is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">Where does the gain come from?</h2>
                        <div class="item">
                            <img src="static/images/ablation_results.png" alt="MY ALT TEXT" />
                            <h2 class="subtitle">
                                Figure 3: Investigation of the influence of CoT \& PoT hybrid training on the 7B Llama-2 model. Key insights include: 1) The SoTA model, utilizing dataset-specific CoT fine-tuning on GSM and MATH, displays strong performance within its domains but struggles
                                in OOD scenarios; 2) Diverse data sources in MathInstruct enable better math generalist model; 3) Fine-tuning on the PoT subsets generally outperforms fine-tuning on the CoT subsets; 4) Hybrid training yields the best-performing
                                model.
                            </h2>
                            <p>
                                In order to better understand what factors contribute to the great gain of ðŸ¦£MAmmoTH over existing baselines, we set up a group of control experiments in the Figure 3. We study the following setups:
                                <ol>
                                    <li>ðŸ¦£<b>MAmmoTH (MathInstruct - CoT):</b> This experiment aims to understand how much our curated CoT data could improve the generalization over the SoTA model WizardMath trained specifically on GSM + MATH. As can be seen,
                                        while sacrificing accuracy on GSM + MATH by 3%, our CoT subset fine-tuning improves the overall nine-dataset accuracy from 27% to 32%. </li>
                                    <li>ðŸ¦£<b>MAmmoTH (MathInstruct - PoT):</b> This experiment aims to understand the advantage of our PoT subset. As can be observed, our PoT subset fine-tuning can significantly improve the overall accuracy from 27% to 37.5%.
                                        This ablation reflects the importance of unlocking the program generation capabilities of our model.</li>
                                    <li>ðŸ¦£<b>MAmmoTH (MathInstruct - Hybrid):</b> We further combine CoT and PoT as the hybrid training data to achieve the best overall performance of 45.4%. This combined gain comes from two aspects:
                                        <ul style="list-style-type: disc;">
                                            <li>
                                                The CoT subset can help maintain the generic language-based reasoning skills to handle scenarios where PoT cannot handle well, e.g., the multi-choice questions in AQuA, SAT, and MMLU.
                                            </li>
                                            <li>
                                                The PoT subset can teach the model how to utilize Python APIs to solve complex math problems with high precision, e.g., the MATH problems requiring complex computation.
                                            </li>
                                        </ul>
                                    </li>
                                </ol>



                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section> -->






    <!-- <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered is-fifths-fifths">
                        <h2 class="title is-3">Species Classification results on iWildCam2020-WILDS (OOD) dataset.</h2>
                        <div class="content has-text-justified">
                            
                            <style type="text/css">
                                .tg  {border-collapse:collapse;border-spacing:0;}
                                .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
                                  overflow:hidden;padding:10px 5px;word-break:normal;}
                                .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
                                  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
                                .tg .tg-baqh{text-align:center;vertical-align:top}
                                .tg .tg-0lax{text-align:left;vertical-align:top}
                                .tg .tg-nrix{text-align:center;vertical-align:middle}
                                </style>
                                <table class="tg">
                                <thead>
                                  <tr>
                                    <th class="tg-0lax" rowspan="2"></th>
                                    <th class="tg-0lax" rowspan="2">Model</th>
                                    <th class="tg-0lax" colspan="3">Multi-modality</th>
                                    <th class="tg-0lax" rowspan="2">Val. Acc.</th>
                                    <th class="tg-0lax" rowspan="2">Test Acc.</th>
                                  </tr>
                                  <tr>
                                    <th class="tg-0lax">Taxonomy</th>
                                    <th class="tg-0lax">Location</th>
                                    <th class="tg-0lax">Time</th>
                                  </tr>
                                </thead>
                                <tbody>
                                  <tr>
                                    <td class="tg-0lax"></td>
                                    <td class="tg-0lax">Empirical Risk Minimization (ERM)</td>
                                    <td class="tg-nrix" colspan="3" rowspan="5">-- <br>&nbsp;&nbsp;<br>&nbsp;&nbsp;<br>&nbsp;&nbsp;<br>&nbsp;&nbsp;</td>
                                    <td class="tg-0lax">62.7 (2.4)</td>
                                    <td class="tg-0lax">71.6 (2.5)</td>
                                  </tr>
                                  <tr>
                                    <td class="tg-0lax"></td>
                                    <td class="tg-0lax">CORAL</td>
                                    <td class="tg-0lax">60.3 (2.8)</td>
                                    <td class="tg-0lax">73.3 (4.3)</td>
                                  </tr>
                                  <tr>
                                    <td class="tg-0lax"></td>
                                    <td class="tg-0lax">Group DRO</td>
                                    <td class="tg-0lax">60.0 (0.7)</td>
                                    <td class="tg-0lax">72.7 (2.0)</td>
                                  </tr>
                                  <tr>
                                    <td class="tg-0lax"></td>
                                    <td class="tg-0lax">Fish</td>
                                    <td class="tg-0lax">58.0 (0.2)</td>
                                    <td class="tg-0lax">63.2 (0.7)</td>
                                  </tr>
                                  <tr>
                                    <td class="tg-0lax"></td>
                                    <td class="tg-0lax">ABSGD</td>
                                    <td class="tg-0lax">--</td>
                                    <td class="tg-0lax">72.7 (1.8)</td>
                                  </tr>
                                  <tr>
                                    <td class="tg-0lax"></td>
                                    <td class="tg-0lax">COSMO (no-context)</td>
                                    <td class="tg-baqh" colspan="3">-- </td>
                                    <td class="tg-0lax">63.2 (0.4)</td>
                                    <td class="tg-0lax">68.8 (2.1)</td>
                                  </tr>
                                  <tr>
                                    <td class="tg-0lax" rowspan="3">Single context</td>
                                    <td class="tg-0lax" rowspan="3">COSMO</td>
                                    <td class="tg-0lax">âœ”</td>
                                    <td class="tg-0lax"></td>
                                    <td class="tg-0lax"></td>
                                    <td class="tg-0lax">62.8 (2.2) (<span style="color:#CB0000">-0.4</span>)</td>
                                    <td class="tg-0lax">72.4 (2.5) (<span style="color:#009901">+3.6</span>)</td>
                                  </tr>
                                  <tr>
                                    <td class="tg-0lax"></td>
                                    <td class="tg-0lax">âœ”</td>
                                    <td class="tg-0lax"></td>
                                    <td class="tg-0lax">64.4 (1.0) (<span style="color:#009901">+1.2</span>)</td>
                                    <td class="tg-0lax">74.5 (3.6) (<span style="color:#009901">+5.7</span>)</td>
                                  </tr>
                                  <tr>
                                    <td class="tg-0lax"></td>
                                    <td class="tg-0lax"></td>
                                    <td class="tg-0lax">âœ”</td>
                                    <td class="tg-0lax">64.7 (0.4) (<span style="color:#009901">+1.5</span>)</td>
                                    <td class="tg-0lax">71.1 (3.1) (<span style="color:#009901">+2.3</span>)</td>
                                  </tr>
                                  <tr>
                                    <td class="tg-0lax" rowspan="4">Multiple contexts</td>
                                    <td class="tg-0lax" rowspan="4"><span style="font-weight:400;font-style:normal">COSMO</span></td>
                                    <td class="tg-0lax">âœ”</td>
                                    <td class="tg-0lax">âœ”</td>
                                    <td class="tg-0lax"></td>
                                    <td class="tg-0lax">65.4 (0.4) (<span style="color:#009901">+2.2</span>)</td>
                                    <td class="tg-0lax">70.4 (2.1) (<span style="color:#009901">+1.6</span>)</td>
                                  </tr>
                                  <tr>
                                    <td class="tg-0lax">âœ”</td>
                                    <td class="tg-0lax"></td>
                                    <td class="tg-0lax">âœ”</td>
                                    <td class="tg-0lax">64.9 (1.6) (<span style="color:#009901">+1.7</span>)</td>
                                    <td class="tg-0lax">73.7 (3.8) (<span style="color:#009901">+4.9</span>)</td>
                                  </tr>
                                  <tr>
                                    <td class="tg-0lax"></td>
                                    <td class="tg-0lax">âœ”</td>
                                    <td class="tg-0lax">âœ”</td>
                                    <td class="tg-0lax">63.0 (2.1) (<span style="color:#CB0000">-0.2</span>)</td>
                                    <td class="tg-0lax">74.2 (2.2) (<span style="color:#009901">+5.4</span>)</td>
                                  </tr>
                                  <tr>
                                    <td class="tg-0lax">âœ”</td>
                                    <td class="tg-0lax">âœ”</td>
                                    <td class="tg-0lax">âœ”</td>
                                    <td class="tg-0lax">65.0 (1.6) (<span style="color:#009901">+1.8</span>)</td>
                                    <td class="tg-0lax">71.5 (2.8) (<span style="color:#009901">+2.7</span>)</td>
                                  </tr>
                                </tbody>
                                </table> -->



    <!-- BibTex citation -->
    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">Citation</h2>
            Please cite our paper if you use our code, data, model or results:
            <br><br>
            <pre><code>@article{pahuja2023bringing,
  title={Bringing Back the Context: Camera Trap Species Identification as Link Prediction on Multimodal Knowledge Graphs},
  author={Pahuja, Vardaan and Luo, Weidi and Gu, Yu and Tu, Cheng-Hao and Chen, Hong-You and Berger-Wolf, Tanya and Stewart, Charles and Gao, Song and Chao, Wei-Lun and Su, Yu},
  journal={arXiv preprint arXiv:2401.00608},
  year={2023}
}
</code></pre>
        </div>
    </section>


    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">

                        <p>
                            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a>                            project page. You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"
                                target="_blank">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
                        </p>

                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>
<style>
    .buttonGroup {
        text-align: center;
    }
    
    .buttonGroup>button {
        padding: 15px;
        color: white;
        background-color: #363636;
        border-radius: 5px;
    }
    
    .buttonGroup>button:hover {
        box-shadow: 5px;
    }
</style>

</html>
